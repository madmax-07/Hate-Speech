{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1338357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics, manifold\n",
    "from tqdm import tqdm\n",
    "import gensim.downloader as gensim_api\n",
    "import transformers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f630b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('output1.csv',nrows=4000)\n",
    "print(df[\"clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = gensim_api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566cf89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.most_similar([\"nigger\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d84534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_words(lst_words, top, nlp):\n",
    "    lst_out = lst_words\n",
    "    for tupla in nlp.most_similar(lst_words, topn=top):\n",
    "        lst_out.append(tupla[0])\n",
    "    return list(set(lst_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c828092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarWords(words, top, nlp):\n",
    "    out = words\n",
    "    for tupl in nlp.most_similar(words, topn=top):\n",
    "        out.append(tupl[0])\n",
    "    return list(set(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea95de",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters={}\n",
    "clusters['HATE']=get_similar_words(['hate'], top=30, nlp=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c7b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('al', do_lower_case=True)\n",
    "nlp = transformers.TFDistilBertModel.from_pretrained('al')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720989e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_bert_embedding(txt, tokenizer, nlp):\n",
    "    '''\n",
    "    Word embedding with Bert (equivalent to nlp[\"word\"]).\n",
    "    :parameter\n",
    "        :param txt: string \n",
    "        :param tokenizer: transformers tokenizer\n",
    "        :param nlp: transformers bert\n",
    "    :return\n",
    "        tensor sentences x words x vector (1x3x768) \n",
    "    '''\n",
    "    # tokenize sentence to tokens (integers)\n",
    "    idx = tokenizer.encode(txt)\n",
    "    # convert to array of shape (1, num_words+2) - EOS and CLS added\n",
    "    idx = np.array(idx)[None,:]\n",
    "    # generate embeddings for each token - output is a tuple\n",
    "    embedding = nlp(idx)\n",
    "    # select first member of the tuple, remove first dimension which is 1 to get (num_words,embedding size 712)\n",
    "    # exclude CLS and EOS tokens\n",
    "    X = np.array(embedding[0][0][1:-1])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead26562",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## create list of news vector\n",
    "lst_mean_vecs = [utils_bert_embedding(txt, tokenizer, nlp).mean(0) for txt in tqdm(df[\"clean\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7f087",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(lst_mean_vecs)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6275923",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"mean_vec.csv\", X, delimiter=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de874b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "my_data = genfromtxt('mean_vec.csv', delimiter='/')\n",
    "print(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c80376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_y = {k:utils_bert_embedding(v, tokenizer, nlp).mean(0) for k,v in tqdm(clusters.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babaae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dic_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb50f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = np.array(\n",
    "            [metrics.pairwise.cosine_similarity(X, y).T.tolist()[0] \n",
    "             for y in dic_y.values()]\n",
    "            ).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d2ca8-bd20-42f5-bfd2-fda19c381fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
